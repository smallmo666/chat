import asyncio
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage
from pydantic import BaseModel, Field
from typing import List, Optional

from src.workflow.state import AgentState
from src.core.llm import get_llm
from src.domain.memory.few_shot import get_few_shot_retriever

class DetectiveResponse(BaseModel):
    is_complex: bool = Field(..., description="Query whether it is complex and needs to be split")
    hypotheses: List[str] = Field(default=[], description="List of hypotheses or sub-questions for complex queries")
    reasoning: str = Field(..., description="Reasoning behind the decision")

DETECTIVE_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªé«˜çº§æ•°æ®ä¾¦æ¢ (Data Detective)ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œåˆ¤æ–­å…¶æ˜¯å¦æ˜¯ä¸€ä¸ªå¤æ‚çš„åˆ†æé—®é¢˜ï¼ˆä¾‹å¦‚æ¶‰åŠå½’å› åˆ†æã€å¼‚å¸¸æ£€æµ‹ã€é¢„æµ‹æˆ–å¤šæ­¥æ¨ç†ï¼‰ã€‚

ç”¨æˆ·æŸ¥è¯¢: {query}

### ä»»åŠ¡æŒ‡å—:
1. **ç®€å•æŸ¥è¯¢**: å¦‚æœç”¨æˆ·åªæ˜¯æŸ¥è¯¢äº‹å®ï¼ˆå¦‚ "ä¸Šå‘¨é”€å”®é¢æ˜¯å¤šå°‘"ã€"æŸ¥è¯¢ iPhone çš„åº“å­˜"ï¼‰ï¼Œæ ‡è®°ä¸º `is_complex=False`ã€‚
2. **å¤æ‚æŸ¥è¯¢**: å¦‚æœç”¨æˆ·è¯¢é—® "ä¸ºä»€ä¹ˆ" (Why)ã€"å¦‚ä½•" (How)ã€"é¢„æµ‹" (Predict) æˆ–æš—ç¤ºäº†éœ€è¦æ·±å…¥æŒ–æ˜ï¼ˆå¦‚ "åˆ†æé”€å”®é¢ä¸‹é™çš„åŸå› "ï¼‰ï¼Œæ ‡è®°ä¸º `is_complex=True`ã€‚
3. **å‡è®¾ç”Ÿæˆ**: å¯¹äºå¤æ‚æŸ¥è¯¢ï¼Œè¯·æå‡º 3-5 ä¸ªå¯èƒ½çš„å‡è®¾æˆ–æ‹†è§£åçš„å­é—®é¢˜ã€‚ä¾‹å¦‚ï¼š
   - ç”¨æˆ·: "ä¸ºä»€ä¹ˆä¸Šå‘¨é”€å”®é¢ä¸‹é™ï¼Ÿ"
   - å‡è®¾: ["æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºè´§æƒ…å†µ", "åˆ†æä¸»è¦ç«å“æ˜¯å¦é™ä»·", "æŸ¥çœ‹ç‰¹å®šåœ°åŒºæˆ–æ¸ é“çš„é”€å”®è¡¨ç°"]

### å‚è€ƒæ¡ˆä¾‹ (Few-Shot):
{few_shot_context}

è¯·è¾“å‡º JSON æ ¼å¼çš„åˆ†æç»“æœã€‚
"""

async def data_detective_node(state: AgentState, config: dict = None) -> dict:
    """
    æ•°æ®ä¾¦æ¢èŠ‚ç‚¹ã€‚
    åœ¨ Planner ä¹‹å‰è¿è¡Œï¼Œè´Ÿè´£è¯†åˆ«å¤æ‚é—®é¢˜å¹¶ç”Ÿæˆåˆ†æå‡è®¾ã€‚
    """
    print("DEBUG: Entering data_detective_node")
    
    project_id = config.get("configurable", {}).get("project_id") if config else None
    llm = get_llm(node_name="DataDetective", project_id=project_id)
    
    # è·å–ç”¨æˆ·æœ€æ–°æŸ¥è¯¢
    messages = state.get("messages", [])
    last_query = ""
    for msg in reversed(messages):
        if msg.type == "human":
            last_query = msg.content
            break
            
    if not last_query:
        return {"next": "Planner"}

    # è·å– Few-Shot ä¸Šä¸‹æ–‡ (å¯é€‰)
    few_shot_context = ""
    try:
        retriever = get_few_shot_retriever(project_id)
        # å°è¯•æ£€ç´¢ç±»ä¼¼çš„å¤æ‚æ¡ˆä¾‹
        few_shot_context = await asyncio.to_thread(retriever.retrieve, last_query)
    except Exception as e:
        print(f"Detective: Failed to retrieve few-shot examples: {e}")

    prompt = ChatPromptTemplate.from_template(DETECTIVE_PROMPT)
    chain = prompt | llm.with_structured_output(DetectiveResponse)
    
    try:
        result = await chain.ainvoke({
            "query": last_query,
            "few_shot_context": few_shot_context
        })
        
        print(f"DEBUG: Detective Analysis: Complex={result.is_complex}, Hypotheses={result.hypotheses}")
        
        if result.is_complex and result.hypotheses:
            # å°†å‡è®¾æ³¨å…¥åˆ°çŠ¶æ€ä¸­ï¼Œä¾› Planner ä½¿ç”¨
            # å¹¶ç”Ÿæˆä¸€æ¡ AIMessage å‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨è¿›è¡Œæ·±åº¦åˆ†æ
            notification = f"ğŸ•µï¸â€â™‚ï¸ è¿™æ˜¯ä¸€ä¸ªå€¼å¾—æ·±å…¥åˆ†æçš„é—®é¢˜ã€‚æˆ‘å°†ä»ä»¥ä¸‹å‡ ä¸ªè§’åº¦å…¥æ‰‹ï¼š\n" + "\n".join([f"- {h}" for h in result.hypotheses])
            return {
                "hypotheses": result.hypotheses,
                "analysis_depth": "deep",
                "messages": [AIMessage(content=notification)]
            }
        else:
            return {
                "analysis_depth": "simple",
                "hypotheses": []
            }
            
    except Exception as e:
        print(f"Detective failed: {e}")
        return {"analysis_depth": "simple"} # Fallback
