import json
from typing import List
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage
from pydantic import BaseModel, Field

from src.workflow.state import AgentState
from src.core.llm import get_llm

class InsightResponse(BaseModel):
    insights: List[str] = Field(default=[], description="List of discovered insights. Empty if nothing interesting.")

INSIGHT_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªé«˜çº§æ•°æ®æ´å¯Ÿä¸“å®¶ (Data Insight Miner)ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åŸºäºç”¨æˆ·çš„æŸ¥è¯¢å’Œ SQL æ‰§è¡Œç»“æœï¼Œä¸»åŠ¨æŒ–æ˜æ•°æ®ä¸­éšè—çš„**æœ‰ä»·å€¼çš„æ´å¯Ÿ**ã€‚

ç”¨æˆ·æŸ¥è¯¢: {query}
SQL: {sql}
ç»“æœæ•°æ® (å‰ {sample_size} æ¡):
{data_sample}

### ä»»åŠ¡æŒ‡å—:
1. **å¯»æ‰¾æƒŠå–œ**: ä¸è¦ä»…ä»…å¤è¿°ç»“æœï¼ˆä¾‹å¦‚"ç»“æœæ˜¾ç¤ºAæ˜¯100"ï¼‰ï¼Œè€Œæ˜¯å¯»æ‰¾**å¼‚å¸¸å€¼ (Outliers)**ã€**æ˜¾è‘—è¶‹åŠ¿ (Trends)**ã€**æå€¼ (Extremes)** æˆ– **åç›´è§‰çš„ç°è±¡**ã€‚
2. **è®²æ•…äº‹**: å°†æ´å¯Ÿè½¬åŒ–ä¸ºç®€çŸ­çš„ä¸šåŠ¡æ•…äº‹ã€‚ä¾‹å¦‚ï¼š"è™½ç„¶æ•´ä½“é”€å”®é¢ä¸‹é™ï¼Œä½† iPhone 15 çš„é”€é‡åè€Œé€†åŠ¿å¢é•¿äº† 20%ã€‚"
3. **ä¿æŒå…‹åˆ¶**: å¦‚æœæ•°æ®å¹³å¹³æ— å¥‡ï¼Œä¸è¦å¼ºè¡Œç¼–é€ æ´å¯Ÿï¼Œè¿”å›ç©ºåˆ—è¡¨å³å¯ã€‚
4. **æ•°é‡é™åˆ¶**: æœ€å¤šè¿”å› 3 æ¡æœ€å…³é”®çš„æ´å¯Ÿã€‚

è¯·è¾“å‡º JSON æ ¼å¼çš„æ´å¯Ÿåˆ—è¡¨ã€‚
"""

async def insight_miner_node(state: AgentState, config: dict = None) -> dict:
    """
    ä¸»åŠ¨æ´å¯ŸæŒ–æ˜èŠ‚ç‚¹ã€‚
    åœ¨ SQL æ‰§è¡ŒæˆåŠŸåï¼Œé’ˆå¯¹å¤æ‚æŸ¥è¯¢ (deep mode) è¿è¡Œã€‚
    **å¢å¼º**: å¦‚æœå‘ç°æœ‰ä»·å€¼çš„æ´å¯Ÿï¼Œç”Ÿæˆä¸€æ¡å¸¦æœ‰é«˜äº®æ ‡è®°çš„ AIMessageã€‚
    """
    print("DEBUG: Entering insight_miner_node")
    
    project_id = config.get("configurable", {}).get("project_id") if config else None
    llm = get_llm(node_name="InsightMiner", project_id=project_id)
    
    # è·å–ä¸Šä¸‹æ–‡
    query = ""
    for msg in reversed(state["messages"]):
        if msg.type == "human":
            query = msg.content
            break
            
    sql = state.get("sql", "")
    results_str = state.get("results", "[]")
    
    try:
        results = json.loads(results_str)
        if not isinstance(results, list) or not results:
            print("InsightMiner: Results empty or invalid, skipping.")
            return {"insights": []}
            
        # é‡‡æ ·æ•°æ®ï¼Œé¿å… Token çˆ†ç‚¸
        sample_size = 20
        data_sample = json.dumps(results[:sample_size], ensure_ascii=False)
        
        prompt = ChatPromptTemplate.from_template(INSIGHT_PROMPT)
        chain = prompt | llm.with_structured_output(InsightResponse)
        
        response = await chain.ainvoke({
            "query": query,
            "sql": sql,
            "data_sample": data_sample,
            "sample_size": sample_size
        })
        
        print(f"DEBUG: InsightMiner found {len(response.insights)} insights: {response.insights}")
        
        if response.insights:
            # æ ¼å¼åŒ–æ´å¯Ÿæ–‡æœ¬
            insight_text = "\n".join([f"ğŸ’¡ **æ´å¯Ÿ {i+1}**: {insight}" for i, insight in enumerate(response.insights)])
            
            # åªæœ‰åœ¨éå¸¸ç¡®å®šæ—¶æ‰ä½œä¸ºæ¶ˆæ¯å‘é€ï¼Œé¿å…æ‰“æ‰°ã€‚
            # è¿™é‡Œæˆ‘ä»¬ä½œä¸ºä¸€æ¡è¡¥å……æ¶ˆæ¯å‘é€ã€‚
            return {
                "insights": response.insights,
                "messages": [AIMessage(content=f"åœ¨åˆ†ææ•°æ®æ—¶ï¼Œæˆ‘å‘ç°äº†ä¸€äº›æœ‰è¶£çš„ç°è±¡ï¼š\n\n{insight_text}")]
            }
            
        return {"insights": []}

    except Exception as e:
        print(f"InsightMiner failed: {e}")
        return {"insights": []}
