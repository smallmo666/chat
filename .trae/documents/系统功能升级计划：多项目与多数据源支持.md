I will implement the migration to PostgreSQL and import the Baidu DuSQL dataset as requested.

### 1. Dependencies & Configuration

* **Install Packages**: Add `psycopg2-binary` (for PostgreSQL) and `modelscope` (to download the DuSQL dataset).

* **Update Environment**: Modify `.env` with the provided PostgreSQL credentials:

  * Host: `159.75.148.55`

  * Port: `5432`

  * User: `postgres`

  * Password: `123456`

* **Update Database Connection**: Modify `src/utils/db.py` to switch from `mysql+pymysql` to `postgresql+psycopg2`.

### 2. Database Cleanup & Schema Management

* **Cleanup Strategy**: The import script will drop existing schemas/tables in the target database to ensure a clean slate (as requested).

* **Schema Strategy**: Since DuSQL contains multiple databases (domains), I will map each DuSQL database to a **PostgreSQL Schema** (e.g., `CREATE SCHEMA banking;`). This avoids table name collisions and keeps domains organized.

### 3. DuSQL Dataset Import

* **Download**: Create a script `src/scripts/import_dusql.py` to download the DuSQL dataset using `modelscope`.

* **Transformation**:

  * Parse the dataset (JSON/SQLite format).

  * Convert SQLite types to PostgreSQL types.

  * Generate DDL (Data Definition Language) for PostgreSQL.

  * Apply Chinese descriptions using `COMMENT ON TABLE` and `COMMENT ON COLUMN`.

* **Data Loading**: Batch insert the data from the dataset into the created PostgreSQL tables.

### 4. Integration

* **Startup Logic**: Update `src/server.py` and `src/utils/db.py` to skip the old synthetic data generation (`ensure_demo_data`) and instead verify/trigger the DuSQL import if the database is empty.

### 5. Verification

* Verify connection to the new PostgreSQL database.

* Confirm that DuSQL schemas and tables are correctly created and populated.

