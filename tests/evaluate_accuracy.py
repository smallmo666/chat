import sys
import os
import json
import asyncio
import pandas as pd
from sqlalchemy import text
from termcolor import colored

# Add project root to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.utils.db import get_query_db
from src.state.state import AgentState
from src.agents.gen_dsl import generate_dsl_node
from src.agents.dsl2sql import dsl_to_sql_node
from langchain_core.messages import HumanMessage

class BenchmarkRunner:
    def __init__(self, cases_path: str = "tests/data/benchmark_cases.json"):
        self.cases_path = cases_path
        with open(cases_path, "r") as f:
            self.cases = json.load(f)
        self.db = get_query_db()
        self.results = []

    async def run_query_on_db(self, sql: str):
        try:
            # ä½¿ç”¨åŒæ­¥æ–¹æ³•ï¼Œå› ä¸º evaluate è„šæœ¬é€šå¸¸ä¸æ˜¯ async é“¾çš„ä¸€ç¯ï¼Œç®€å•èµ·è§
            # ä½† QueryDatabase ç°åœ¨æ”¯æŒ asyncã€‚æˆ‘ä»¬è¿™é‡Œä½¿ç”¨ run_query (sync wrapper) æ–¹ä¾¿å¯¹æ¯”
            # æˆ–è€…ä½¿ç”¨ async run_query_async
            res = await self.db.run_query_async(sql)
            if res.get("error"):
                return None, res["error"]
            # è§£æ JSON ç»“æœ
            data = json.loads(res["json"])
            return data, None
        except Exception as e:
            return None, str(e)

    def compare_results(self, res1, res2) -> bool:
        """
        æ¯”è¾ƒä¸¤ä¸ªç»“æœé›†æ˜¯å¦ä¸€è‡´ (å¿½ç•¥é¡ºåº)ã€‚
        """
        if res1 is None or res2 is None:
            return False
        
        # ç®€å•æ¯”è¾ƒï¼šè½¬æ¢ä¸º DataFrame ç„¶åæ’åºæ¯”è¾ƒ
        try:
            df1 = pd.DataFrame(res1)
            df2 = pd.DataFrame(res2)
            
            if df1.empty and df2.empty:
                return True
            if df1.shape != df2.shape:
                return False
                
            # ç»Ÿä¸€åˆ—åå¤§å°å†™?
            # æ’åº
            df1_sorted = df1.sort_values(by=list(df1.columns)).reset_index(drop=True)
            df2_sorted = df2.sort_values(by=list(df2.columns)).reset_index(drop=True)
            
            return df1_sorted.equals(df2_sorted)
        except Exception:
            return False

    async def run(self):
        print(colored(f"ğŸš€ Starting Benchmark: {len(self.cases)} cases", "cyan", attrs=["bold"]))
        
        passed = 0
        
        for i, case in enumerate(self.cases):
            q = case["question"]
            expected_sql = case["expected_sql"]
            print(f"\n[{i+1}/{len(self.cases)}] Testing: {q}")
            
            # 1. Generate SQL (Simulation)
            # æˆ‘ä»¬æ¨¡æ‹Ÿ Graph çš„ä¸€éƒ¨åˆ†ï¼šGenerateDSL -> DSLtoSQL
            state = AgentState(messages=[HumanMessage(content=q)])
            
            # Mock config
            config = {"configurable": {"project_id": 1}} # Assuming project 1
            
            try:
                # Step 1: Gen DSL
                state_dsl = generate_dsl_node(state, config)
                dsl = state_dsl.get("dsl")
                state["dsl"] = dsl
                
                # Step 2: DSL to SQL
                state_sql = dsl_to_sql_node(state, config)
                generated_sql = state_sql.get("sql")
                
                print(f"   Generated SQL: {generated_sql}")
                
                # 2. Execute Both
                print("   Executing Expected SQL...")
                expected_res, err1 = await self.run_query_on_db(expected_sql)
                if err1:
                    print(colored(f"   âš ï¸ Expected SQL Failed: {err1}", "yellow"))
                    # å¦‚æœæ ‡å‡†ç­”æ¡ˆéƒ½è·‘ä¸é€šï¼Œå¯èƒ½æ˜¯ç¯å¢ƒé—®é¢˜ï¼Œè·³è¿‡
                    continue

                print("   Executing Generated SQL...")
                gen_res, err2 = await self.run_query_on_db(generated_sql)
                
                if err2:
                    print(colored(f"   âŒ Execution Failed: {err2}", "red"))
                    self.results.append({"case": q, "status": "exec_error", "error": err2})
                    continue
                
                # 3. Compare
                is_match = self.compare_results(expected_res, gen_res)
                
                if is_match:
                    print(colored("   âœ… PASS", "green"))
                    passed += 1
                    self.results.append({"case": q, "status": "pass"})
                else:
                    print(colored("   âŒ FAIL (Result Mismatch)", "red"))
                    print(f"   Expected: {len(expected_res)} rows, Got: {len(gen_res)} rows")
                    self.results.append({"case": q, "status": "mismatch", "generated_sql": generated_sql})

            except Exception as e:
                print(colored(f"   âŒ System Error: {e}", "red"))
                self.results.append({"case": q, "status": "system_error", "error": str(e)})

        accuracy = (passed / len(self.cases)) * 100
        print(colored(f"\nğŸ“Š Benchmark Finished. Accuracy: {accuracy:.2f}% ({passed}/{len(self.cases)})", "cyan", attrs=["bold"]))

if __name__ == "__main__":
    runner = BenchmarkRunner()
    asyncio.run(runner.run())
